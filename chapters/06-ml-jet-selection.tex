\chapter{Machine Learning Based Jet Assignment}
\label{chap:ml-jet-selection}

In addition to the conventional jet assignment methods described in \Cref{sec:jet_assignment}, this thesis investigates machine learning-based approaches to improve the accuracy of jet assignment in dileptonic \ttbar events. Machine learning techniques have shown promise in various aspects of high-energy physics, such as flavour tagging \cite{flavour_tagging_review}.

For the high permutation complexity of jet assignments in the all-jets channel, SPANet \cite{spanet} has demonstrated significant improvements over traditional methods. These architectures leverage attention mechanisms to improve the accuracy.
Based on these successes, this thesis explores the adaptation of similar architectures for the dileptonic \ttbar channel, which presents unique challenges due to the presence of two neutrinos and fewer jets.

Albeit a variety of architectures were tested during the development (including RNNs), two manifested as the best performing ones.

\section{Neural Network Architectures}
\subsection{FeatureConcatTransformer}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.\textwidth]{figures/machine-learning/feature_concat_transformer.pdf}
    \caption{Schematic of the \textbf{FeatureConcatTransformer} architecture.}
    \label{fig:feature_concat_transformer}
\end{figure}
The \textbf{FeatureConcatTransformer} architecture, illustrated in \Cref{fig:feature_concat_transformer} consists of the following components:
\begin{itemize}
    \item \textbf{Input Features:} The model takes as input a set of features for each reconstructed object (jets and leptons), including kinematic variables ($E$, \pt, $\eta$, $\phi$), $b$-tag score working point, and event-level features such as missing transverse energy (\MET). The features of both leptons and the missing energy are concatenated to each jet's feature vector to provide context. 
    \item \textbf{Embedding DNNs:} The concatenated feature sequence is processed through a shared embedding DNN to transform the raw features into a higher-dimensional representation. This size of the embedding is a hyperparameter of the model, called the \textit{embedding dimension}. A default of $3$ layers is used for the DNNs, where the dimension increases exponentially, to match the target dimension.
    \item \textbf{Transformer Layers:} The embedded features are passed through multiple transformer layers, which utilize self-attention mechanisms to capture the relationships between different reconstructed objects.
    \item \textbf{Assignment DNNs:} The output from the transformer layers is fed into assignment DNNs that produce assignment scores for each possible jet assignment to the parton-level decay products. Here too, a default of $3$ layers is used for the DNNs, with exponentially decreasing dimensions.
    \item \textbf{Softmax Layer:} Finally, a softmax layer is applied to the assignment scores for each $b$-quark to obtain probabilities.
\end{itemize}
While the model has several hyperparameters, only the two most important (\textit{number of transformer layers} and \textit{embedding dimension}) are optimized.
Due to the properties of the attention mechanism, the model is inherently permutation equivariant with respect to the jet ordering.

The structure of this architecture allows to easily concatenate additional features to each jet. The fixed structure of the inputs and the output, without any cross-attention mechanism, allows to easily add relational information between the jets and leptons through these additional features.

\subsection{CrossAttentionTransformer}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.\textwidth]{figures/machine-learning/cross_attention_transformer.pdf}
    \caption{Schematic of the \textbf{CrossAttentionTransformer} architecture. The annotations (Q$:=$Query, K$:=$Key, V$:=$Value) describe the inputs used for the in attention mechanism following the notation introduced in \Cref{sec:transformer}}
    \label{fig:cross_attention_transformer}
\end{figure}
The \textbf{CrossAttentionTransformer} architecture, illustrated in \Cref{fig:cross_attention_transformer}, consists of the following components:
\begin{itemize}
    \item \textbf{Input Features:} 
    \begin{itemize}
        \item \textbf{Jet Inputs:} The jet inputs are ($E$, \pt, $\eta$, $\phi$), $b$-tag score working point, concatenated with event level features such as \met
        \item \textbf{Lepton Inputs} The leptons inputs are their kinematic features ($E$, \pt, $\eta$, $\phi$) and the \met features.
    \end{itemize}
    \item \textbf{Embedding DNNs:} The jet and lepton sequences are each embedded using their own shared DNN with $3$ layers, that exponentially increase the dimension to match the \textit{embedding dimension}.
    \item \textbf{Self-Attention Encoder:} Both the jet and lepton embeddings are processed through their own self-attention transformer encoders, each consisting of multiple layers. This allows the model to capture intra-object relationships within jets and leptons separately. The \textit{number of encoder layers} is a hyperparameter of the model.
    \item \textbf{Cross-Attention Decoder:} The outputs from the jet and lepton encoders are then fed into a cross-attention decoder. Here, the jet embeddings serve as the Query (Q) inputs, while the lepton embeddings provide the Key (K) and Value (V) inputs. This mechanism enables the model to learn inter-object relationships between jets and leptons effectively. The \textit{number of decoder layers} is a hyperparameter of the model. To simplify the architecture, the same number of layers is used for both the encoder and decoder.\\
    The final layer of the cross-attention decoder outputs is used for the subsequent assignment.
\end{itemize}
This model has several hyperparameters, to keep the optimization feasible, only the two most important (\textit{number of encoder/decoder layers} and \textit{embedding dimension}) are optimized.
Similar to the \textbf{FeatureConcatTransformer}, this model is also permutation equivariant with respect to the jet ordering, but due to the cross-attention mechanism it is additionally permutation equivariant with respect to the lepton ordering.

While, the two architectures share similarities, the \textbf{CrossAttentionTransformer} explicitly models the relationships between jets and leptons through the cross-attention mechanism, while the \textbf{FeatureConcatTransformer} relies on concatenated features to provide context. This structural difference may lead to varying performance depending on the complexity of the relationships in the data.

\section{Training Procedure}
Both models are trained using supervised learning. The training dataset consists of simulated dileptonic \ttbar events, where the true jet-to-parton assignments are known from the parton matching information. Both models are trained to minimize the categorical cross-entropy loss between the predicted assignment probabilities and the true assignments for each $b$-quark. The AdamW \cite{adamW} optimizer is used with a learning rate of $10^{-4}$ and a weight decay of $10^{-4}$. Further, a dropout with a rate of $r=0.1$ is applied to all internal forward passes during training. The models are trained for $50$ epochs with a batch size of $1024$ events, using early stopping based on the validation loss to prevent overfitting.
\subsection{Metrics}
\label{sec:performance-metrics}
The primary metric used to evaluate the performance of the jet assignment models is the \textbf{assignment accuracy}, defined as the fraction of events where both $b$-jets are correctly assigned to their respective parton-level $b$-quarks. Additionally, the \textbf{selection accuracy} is reported, which measures the fraction of events where the two selected jets correspond to the true $b$-quarks, regardless of the specific assignment.

Another useful metric, especially when comparing different methods across varying event conditions, is the \textbf{accuracy quotient}. This metric is the ratio of the assignment accuracy to the selection accuracy. It provides insight into how well a method performs the assignment task given that the correct jets have been selected, effectively normalizing out the jet selection performance.



\section{Hyperparameter Optimization}
\label{sec:hyperparameter-optimisation}
\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1.\textwidth]{plots/grid_search_analysis/CrossAttentionTransformer/grid_search_accuracy_heatmap.pdf}
        \caption{\textbf{CrossAttentionTransformer}}
        \label{fig:hyperparameter_optimization_cross_attention_transformer_accuracy}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1.\textwidth]{plots/grid_search_analysis/FeatureConcatTransformer/grid_search_accuracy_heatmap.pdf}
        \caption{\textbf{FeatureConcatTransformer}}
        \label{fig:hyperparameter_optimization_feature_concat_transformer_accuracy}
    \end{subfigure}
    \caption{Assignment accuracy on the validation dataset for different hyperparameter combinations for \subrefb{fig:hyperparameter_optimization_cross_attention_transformer_accuracy} the \textbf{CrossAttentionTransformer} and \subrefb{fig:hyperparameter_optimization_feature_concat_transformer_accuracy} the \textbf{FeatureConcatTransformer} architectures. Each square represents a model trained with the corresponding hyperparameter combination.}
    \label{fig:hyperparameter_optimization_accuracy}
\end{figure}
Both architectures have multiple hyperparameters, that can be varied to tune the model performance. Increasing the model complexity generally improves the performance, but also increases the training time, memory requirements, and the risk of overfitting. While an extensive hyperparameter optimization is beyond the scope of this thesis. Given the size of the dataset of about $20$ million \ttbar events in the nominal \atlas sample, the computational limits could not be fully explored.

But to get a first estimate of the behaviour of the models with respect to their hyperparameters, a grid search over two of the most important hyperparameters for each architecture was performed.
For the \textbf{FeatureConcatTransformer}, the \textit{number of transformer layers} and the \textit{embedding dimension} were varied, while for the \textbf{CrossAttentionTransformer}, the \textit{number of encoder/decoder layers} and the \textit{embedding dimension} were varied. Again, due to computational limits, for this thesis, only one value for each hyperparameter could be tested. For the final thesis, a more extensive hyperparameter optimization is planned, using k-fold cross-validation to ensure robust performance estimates.

The results of the grid search for the \textbf{CrossAttentionTransformer} are depicted in \Cref{fig:hyperparameter_optimization_accuracy}. Due to the single run per hyperparameter combination, the results are subject to statistical fluctuations. However, a clear trend of increasing accuracy with larger embedding dimensions is visible. The number of layers seems to have a smaller impact on the performance, with only a slight increase in accuracy for more layers and sometimes even a decrease for larger models. This can be explained, if ones considers, that the model performance is highly dependent on the number of parameters, which increases with the square of the embedding dimension but only linearly with the transformer stack size.
A plot showing the number of trainable parameters for each hyperparameter combination is in the Appendix in \Cref{fig:hyperparameter_optimization_params}.

In \Cref{fig:hyperparameter_optimization_efficiency} the accuracy for each hyperparameter combination is shown against the number of trainable parameters. Here, a clear trend of increasing accuracy with more parameters is visible for both architectures. For both models, it can be seen, that the performance gain starts to saturate for larger models, indicating that further increasing the model complexity might not yield significant improvements.
\begin{figure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1.\textwidth]{plots/grid_search_analysis/CrossAttentionTransformer/grid_search_efficiency.pdf}
        \caption{\textbf{CrossAttentionTransformer}}
        \label{fig:hyperparameter_optimization_cross_attention_transformer_efficiency}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1.\textwidth]{plots/grid_search_analysis/FeatureConcatTransformer/grid_search_efficiency.pdf}
        \caption{\textbf{FeatureConcatTransformer}}
        \label{fig:hyperparameter_optimization_feature_concat_transformer_efficiency}
    \end{subfigure}
    \caption{Assignment accuracy on the validation dataset for different hyperparameter combinations for \subrefb{fig:hyperparameter_optimization_cross_attention_transformer_efficiency} the \textbf{CrossAttentionTransformer} and \subrefb{fig:hyperparameter_optimization_feature_concat_transformer_efficiency} the \textbf{FeatureConcatTransformer} architectures. Each dot represents a model trained with the corresponding hyperparameter combination. The dots are coloured according to the embedding dimension used in the model.}
    \label{fig:hyperparameter_optimization_efficiency}
\end{figure}
\section{Performance Comparison}
\begin{table}[H]
    \centering
    \input{plots/plot_HLF_comp_histories/reconstruction_accuracies_table}
    \caption{Jet assignment accuracies and selection accuracies for each of the machine learning architectures. Each best performing hyperparameter combination from the grid search is used for the architectures. The \textbf{FeatureConcatTransformer + HLF} model includes additional high-level features as input. The uncertainties are statistical only and stem from bootstrap resampling. The validation sample size is about $2$ million events, while the test sample size is about $4$ million events.}
    \label{tab:ml_jet_assignment_performance}
\end{table}
For the overall performance comparison, the best performing hyperparameter combination from the grid search is selected for each architecture. The assignment accuracies and selection accuracies on the test dataset are summarized in \Cref{tab:ml_jet_assignment_performance}\footnote{Note, that the errors only account for the variance within the data samples itself. Meaning, they only serve to make statements on the performance of the isolated methods and especially their comparison. However, they do not allow to quantify the performance one would expect in an analysis pipline.}. It can be seen, that there is only a small difference in performance between the two architectures, with the \textbf{FeatureConcatTransformer} performing slightly better than the \textbf{CrossAttentionTransformer}. Note, that even though, these numbers come with errors, they are statistical only and do not account for model variance. For the selection accuracies, both models achieve similar performance, indicating that they are equally effective at selecting the correct jets, even if the specific assignment differs.
While both architectures provide promising performance, the \textbf{FeatureConcatTransformer} demonstrates a slight edge in assignment accuracy, making it the preferred choice for further studies in this thesis.

Additional plots evaluating the performance of both architectures are included in the Appendix in \Cref{fig:binned_accuracy_model_architectures_n_jets,fig:binned_accuracy_model_architectures_ttbar_mass,fig:binned_accuracy_model_architectures_ttbar_pT}.
\subsection{High Level Input Features}
As mentioned before, the variables used as input features for the models are primarily low-level kinematic variables of the reconstructed objects. However, high-level features, derived from physics insights, can provide additional context and improve model performance. Examples of such high-level features (HLF) include angular separations between jets and leptons $\Delta R(\ell,\text{jet})$, invariant masses of jet-lepton $m(\ell,\text{jet})$ pairs. These variables are known to be sensitive to the correct jet assignment in \ttbar events. The  corresponding distributions of the added high-level features for correct and incorrect assignments are shown in the Appendix in \Cref{fig:relational_jet_lepton_deltaR,fig:relational_jet_lepton_invariant_mass}.

The \textbf{FeatureConcatTransformer} architecture is particularly well-suited for incorporating these high-level features, as they can be easily concatenated to the jet feature vectors. The \textbf{CrossAttentionTransformer} cannot relate these features based on their position in the input features to the leptons, due to its symmetry in the lepton ordering. This limits the usability of these features in this architecture.
To evaluate the impact of high-level features, the \textbf{FeatureConcatTransformer} model is retrained with $\Delta R$ and $m(\ell,\text{jet})$ added to each jet's feature vector. The model is trained and evaluated using the same procedure as before.
The results, summarized in \Cref{tab:ml_jet_assignment_performance}, indicate that the inclusion of high-level features leads to a noticeable improvement in assignment accuracy, while the selection accuracy remains relatively unchanged. This suggests that the high-level features provide valuable information that helps the model make more accurate assignments. Interestingly, the selection accuracy slightly decreases when high-level features are included, which could be due to the model focusing more on specific assignments rather than just selecting the correct jets. Overall, incorporating high-level features proves beneficial for enhancing the jet assignment performance of the \textbf{FeatureConcatTransformer} model. Plots displaying the hyperparameter optimization results with high-level features are included in the Appendix in \Cref{fig:hyperparameter_optimization_feature_concat_transformer_HLF}.
\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{plots/plot_HLF_comp_histories/training_histories_comparison.png}
    \caption{Training histories of both architectures and potentially high-level features. The left plot shows the value of the loss-function and the right plot shows the assignment accuracy. For each model, the dotted line corresponds to the validation data.}
    \label{fig:training_histories_comparison}
\end{figure}
The training histories of both architectures, with and without high-level features, can be seen in Figure \ref{fig:training_histories_comparison}. It can be seen, that the \textbf{FeatureConcatTransformer} with high-level features converges faster and achieves a lower validation loss compared to the other models. This indicates that the high-level features help the model learn more effectively, leading to improved performance. Another intestering feature, can be seen in the training history of the \textbf{CrossAttentionTransformer}, where a significant gap between the training and validation accuracy is visible, indicating potential overfitting. In contrast, the \textbf{FeatureConcatTransformer} models deliver a better generalization to the validation data, suggesting that this architecture is more robust against overfitting in this context. For these models, the validation performs slightly better than the training, which is explained by the dropout layers, which are only active during training.
Lastly, the \textbf{FeatureConcatTransformer} without high-level features exhibits a more stable training process compared to the \textbf{CrossAttentionTransformer}, further highlighting its suitability for this task.
But overall, the \textbf{FeatureConcatTransformer} with high-level features demonstrates the most stable and effective training behaviour, leading to superior performance in jet assignment tasks.
\subsection{Feature Evaluation}
\label{sec:feature_evaluation}
\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1.\textwidth]{plots/plot_HLF_comp_histories/FeatureConcatTransformer_feature_importance..pdf}
        \caption{\textbf{FeatureConcatTransformer}}
        \label{fig:feature_importance_feature_concat_transformer}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1.\textwidth]{plots/plot_HLF_comp_histories/CrossAttentionTransformer_feature_importance..pdf}
        \caption{\textbf{CrossAttentionTransformer}}
        \label{fig:feature_importance_cross_attention_transformer}
    \end{subfigure}\\[1em]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1.\textwidth]{plots/plot_HLF_comp_histories/FeatureConcatTransformer HLF_feature_importance..pdf}
        \caption{\centering\textbf{FeatureConcatTransformer}\\
        with HLF}
        \label{fig:feature_importance_feature_concat_transformer_HLF}
    \end{subfigure}
    \caption{The feature importances for each of the three models evaluated using permutation importance \cite{permutation_importance}. The importance is calculated as the decrease in assignment accuracy when a specific feature is randomly permuted. Particle features are permutated over all particles. Higher values indicate more important features.}
    \label{fig:feature_importance}
\end{figure}
To gain insights into the decision-making process of the models, permutation importance \cite{permutation_importance} is used to evaluate the importance of each input feature. This method involves randomly permuting each feature and measuring the decrease in assignment accuracy, providing a quantitative measure of how much each feature contributes to the model's performance.
The feature importances for each of the three models are depicted in \Cref{fig:feature_importance}. For all models, the $b$-tagging scores emerge as the most important features, underscoring their critical role in jet assignment tasks. Kinematic features such as $\phi$ and $\eta$ show significant importance, indicating that the model leverages these variables to distinguish between jets effectively. The missing transverse energy (\met) has a negligible importance, suggesting that its contribution to jet assignment is minimal in this context.

When high-level features are included in the \textbf{FeatureConcatTransformer}, they exhibit substantial importance, particularly the angular separation $m(\ell,\text{jet})$. This highlights that these derived features provide valuable context that enhances the model's ability to make accurate assignments. Additionally, the importance of the low-level features, especially the $\eta$ of both jets and leptons, is negligible. This suggests that the high-level features effectively capture the relevant information, reducing the reliance on certain low-level kinematic variables.

\subsection{Summary}
In conclusion, both the \textbf{FeatureConcatTransformer} and \textbf{CrossAttentionTransformer} architectures demonstrate promising performance in jet assignment tasks for dileptonic \ttbar events. The \textbf{FeatureConcatTransformer} seems to have a slight edge in assignment accuracy, particularly when high-level features are incorporated. The training histories indicate that the \textbf{FeatureConcatTransformer} converges more effectively and generalizes better to validation data, suggesting its robustness against overfitting. Further, this model architecture allows to easily embed additional physics-inspired features, which can significantly enhance its performance. While, from a physics point of view, the \textbf{CrossAttentionTransformer} seems more appealing due to its symmetry properties, the \textbf{FeatureConcatTransformer} proves to be more effective in practice for this specific task. One reason for this could be, that the cross-attention mechanism is more limited in capturing broader event context, as it focuses on pairwise interactions between jets and leptons, potentially overlooking more complex relationships that the \textbf{FeatureConcatTransformer} can capture through its concatenated feature approach.
Apart from that, even though from a physics perspective the process is symmetric with respect to the lepton charge, in practice, due to differences in how the detector can resolve leptons of different charges, this symmetry is not exact. Which is why many approaches involving leptons often include the charge as an additional feature. This would similarly break the symmetry in the lepton ordering, thus making the use of a model that does not have this symmetry less of a drawback.